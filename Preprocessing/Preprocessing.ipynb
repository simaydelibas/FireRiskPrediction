{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Combining cvs data from 4 different regions downloaded with GEE"
      ],
      "metadata": {
        "id": "lR1J3YIT6k3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "csv_paths = {\n",
        "    \"Region1\": \"FireRisk_Region1_NDVI_NBR_Burned.csv\",\n",
        "    \"Region2\": \"FireRisk_Region2_NDVI_NBR_Burned.csv\",\n",
        "    \"Region3\": \"FireRisk_Region3_NDVI_NBR_Burned.csv\",\n",
        "    \"Region4\": \"FireRisk_Region4_NDVI_NBR_Burned.csv\"\n",
        "}\n",
        "\n",
        "df_list = []\n",
        "for region, path in csv_paths.items():\n",
        "    if os.path.exists(path):\n",
        "        df = pd.read_csv(path)\n",
        "        df[\"Region\"] = region\n",
        "        df_list.append(df)\n",
        "\n",
        "df_all = pd.concat(df_list, ignore_index=True)\n",
        "df_all.to_csv(\"FireRisk_Combined_AllRegions.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "1MrzIKzm5oCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Demonstrates how MODIS NDVI/NBR values were matched to image coordinates and dates"
      ],
      "metadata": {
        "id": "wme7WVYT7bki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def match_dates(target_path, output_path):\n",
        "    df_target = pd.read_csv(target_path)\n",
        "    coords = df_target[[\"lon\", \"lat\"]].values\n",
        "    distances, indices = nn_model.kneighbors(coords)\n",
        "    matched_dates = df_ndvi.iloc[indices.flatten()].reset_index(drop=True)\n",
        "    df_target[\"date\"] = matched_dates[\"date\"]\n",
        "    df_target.to_csv(output_path, index=False)\n"
      ],
      "metadata": {
        "id": "fMt2H1X97ab1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample Snippet: Matching FIRMS Fire Data"
      ],
      "metadata": {
        "id": "u6G1Jf19_NQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import timedelta\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Load FIRMS fire data\n",
        "df_firms = pd.read_csv(\"fire_data_final.csv\")\n",
        "df_firms = df_firms.dropna(subset=[\"latitude\", \"longitude\", \"datetime\"])\n",
        "df_firms[\"datetime\"] = pd.to_datetime(df_firms[\"datetime\"])\n",
        "\n",
        "# Matching function\n",
        "def match_firms(input_path, output_path, tolerance=3):\n",
        "    df_target = pd.read_csv(input_path)\n",
        "    df_target[\"date\"] = pd.to_datetime(df_target[\"date\"])\n",
        "\n",
        "    matched_records = []\n",
        "\n",
        "    for _, row in df_target.iterrows():\n",
        "        t_date = row[\"date\"]\n",
        "        t_lon, t_lat = row[\"lon\"], row[\"lat\"]\n",
        "\n",
        "        mask = (\n",
        "            (df_firms[\"datetime\"] >= t_date - timedelta(days=tolerance)) &\n",
        "            (df_firms[\"datetime\"] <= t_date + timedelta(days=tolerance))\n",
        "        )\n",
        "        df_sub = df_firms[mask]\n",
        "\n",
        "        if df_sub.empty:\n",
        "            matched_records.append({\n",
        "                \"frp\": None,\n",
        "                \"brightness\": None,\n",
        "                \"confidence\": None\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        model = NearestNeighbors(n_neighbors=1)\n",
        "        model.fit(df_sub[[\"longitude\", \"latitude\"]])\n",
        "        _, idx = model.kneighbors([[t_lon, t_lat]])\n",
        "        nearest = df_sub.iloc[idx[0][0]]\n",
        "\n",
        "        matched_records.append({\n",
        "            \"frp\": nearest[\"frp\"],\n",
        "            \"brightness\": nearest[\"brightness\"],\n",
        "            \"confidence\": nearest[\"confidence\"]\n",
        "        })\n",
        "\n",
        "    df_result = pd.concat(\n",
        "        [df_target.reset_index(drop=True), pd.DataFrame(matched_records)],\n",
        "        axis=1\n",
        "    )\n",
        "    df_result.to_csv(output_path, index=False)\n",
        "    print(f\"Saved with FIRMS: {output_path}\")\n"
      ],
      "metadata": {
        "id": "40BpYN4U_FG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample Snippet of Climate Processing"
      ],
      "metadata": {
        "id": "uxVV51-w9dBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xarray as xr\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load instant and accumulated ERA5 NetCDF files\n",
        "ds_instant = xr.open_dataset(\"instant.nc\")\n",
        "ds_accum = xr.open_dataset(\"accum.nc\")\n",
        "\n",
        "# Extract variables\n",
        "t2m = ds_instant[\"t2m\"].values - 273.15\n",
        "u10 = ds_instant[\"u10\"].values\n",
        "v10 = ds_instant[\"v10\"].values\n",
        "wind_mps = np.sqrt(u10**2 + v10**2)\n",
        "tp = ds_accum[\"tp\"].values\n",
        "\n",
        "# Flatten into tabular structure\n",
        "times = pd.to_datetime(ds_instant[\"valid_time\"].values).normalize()\n",
        "lats = ds_instant[\"latitude\"].values\n",
        "lons = ds_instant[\"longitude\"].values\n",
        "time_grid, lat_grid, lon_grid = np.meshgrid(times, lats, lons, indexing=\"ij\")\n",
        "\n",
        "df_era = pd.DataFrame({\n",
        "    \"date\": time_grid.ravel(),\n",
        "    \"lat\": lat_grid.ravel(),\n",
        "    \"lon\": lon_grid.ravel(),\n",
        "    \"temp_c\": t2m.ravel(),\n",
        "    \"wind_mps\": wind_mps.ravel(),\n",
        "    \"precip_m\": tp.ravel()\n",
        "}).dropna()\n"
      ],
      "metadata": {
        "id": "HWrCwyq59OzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Domain-Inspired Features"
      ],
      "metadata": {
        "id": "h_pOP3-i684w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Apf65RWj3hPp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "train_path = \"Matched_Train_withFIRMS_ERAS_fixed_final.csv\" ## Merged MODIS, FIRMS, and ERA5 features\n",
        "df = pd.read_csv(train_path)\n",
        "\n",
        "# Derived features\n",
        "df[\"burn_count\"] = df.groupby([\"lat\", \"lon\"])[\"Burned\"].transform(\"sum\")\n",
        "df[\"fire_potential\"] = df[\"temp_c\"] * df[\"precip_m\"]\n",
        "df[\"frp_wind_conf\"] = df[\"frp\"] * df[\"wind_mps\"] * df[\"confidence\"]\n",
        "df[\"burned_veg_loss\"] = df[\"Burned\"] * (df[\"NDVI\"] - df[\"NBR\"])\n",
        "df[\"high_heat_stress\"] = df[\"temp_c\"] * (1 - df[\"NDVI\"])\n",
        "df[\"risk_persistence\"] = df[\"burn_count\"] * df[\"fire_potential\"]\n",
        "\n",
        "# Inspect\n",
        "print(df[[\"burn_count\", \"fire_potential\", \"frp_wind_conf\", \"risk_persistence\",\n",
        "          \"burned_veg_loss\", \"high_heat_stress\"]].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating final CVs by bringing all the features together"
      ],
      "metadata": {
        "id": "xS5k8HTM7Glv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_and_save_final_features(train_path, val_path=None, test_path=None, save=True):\n",
        "    def process(df):\n",
        "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
        "        df[\"burn_count\"] = df.groupby([\"lat\", \"lon\"])[\"Burned\"].transform(\"sum\")\n",
        "\n",
        "        if \"wind_mps\" in df.columns and \"confidence\" in df.columns:\n",
        "            df[\"wind_conf\"] = df[\"wind_mps\"] * df[\"confidence\"]\n",
        "        else:\n",
        "            df[\"wind_conf\"] = None\n",
        "\n",
        "        if \"frp\" in df.columns and \"Burned\" in df.columns:\n",
        "            df[\"fire_potential\"] = df[\"frp\"] * df[\"Burned\"]\n",
        "        else:\n",
        "            df[\"fire_potential\"] = None\n",
        "\n",
        "        df[\"risk_persistence\"] = 1 - df[\"frp\"]\n",
        "        df[\"risk_factor\"] = df[\"burn_count\"] + df[\"fire_potential\"] + df[\"risk_persistence\"]\n",
        "\n",
        "        df[\"classname\"] = df[\"class\"]\n",
        "\n",
        "        final_cols = [\n",
        "            \"filename\", \"classname\", \"class\", \"lat\", \"lon\", \"date\",\n",
        "            \"NDVI\", \"NBR\", \"Burned\", \"frp\", \"brightness\", \"confidence\",\n",
        "            \"temp_c\", \"wind_mps\", \"precip_m\",\n",
        "            \"burn_count\", \"fire_potential\", \"wind_conf\",\n",
        "            \"risk_factor\", \"risk_persistence\"\n",
        "        ]\n",
        "\n",
        "        return df[final_cols]\n",
        "\n",
        "    df_train, df_val, df_test = None, None, None\n",
        "\n",
        "    if train_path:\n",
        "        df_train = pd.read_csv(train_path)\n",
        "        df_train_final = process(df_train)\n",
        "        if save:\n",
        "            df_train_final.to_csv(\"train_Final.csv\", index=False)\n",
        "\n",
        "    if val_path:\n",
        "        df_val = pd.read_csv(val_path)\n",
        "        df_val_final = process(df_val)\n",
        "        if save:\n",
        "            df_val_final.to_csv(\"val_Final.csv\", index=False)\n",
        "\n",
        "    if test_path:\n",
        "        df_test = pd.read_csv(test_path)\n",
        "        df_test_final = process(df_test)\n",
        "        if save:\n",
        "            df_test_final.to_csv(\"test_Final.csv\", index=False)\n",
        "\n",
        "    return df_train_final, df_val_final, df_test_final\n",
        "\n",
        "train_path = \"Matched_Train_withFIRMS_ERAS_fixed_final.csv\"\n",
        "val_path = \"Matched_Val_withFIRMS_ERAS_fixed_final.csv\"\n",
        "test_path = \"Matched_Test_withFIRMS_ERAS_fixed_final.csv\"\n",
        "\n",
        "df_train, df_val, df_test = create_and_save_final_features(\n",
        "    train_path, val_path, test_path, save=True\n",
        ")\n",
        "\n",
        "import shutil\n",
        "output_path = \"...\"\n",
        "\n",
        "shutil.copy(\"train_Final.csv\", drive_dir + \"train_Final.csv\")\n",
        "shutil.copy(\"val_Final.csv\", drive_dir + \"val_Final.csv\")\n",
        "shutil.copy(\"test_Final.csv\", drive_dir + \"test_Final.csv\")"
      ],
      "metadata": {
        "id": "bigtrDk54DYH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}